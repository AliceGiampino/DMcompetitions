---
title: "OkCupid"
output: html_document
---

NAME: Alice Giampino 

BADGE: 790347

NICKNAME: a.giampino 

TEAM: FixedMind 

ROUND: 1st

### Summary:

La nostra strategia è stata:

1. Analisi esplorativa delle variabili
2. Feature engineering successivamente scartato
3. Valutazione prestazione modelli con up e down sampling
4. Random forest ranger down sampling


### References:

* Copy/paste from caret html [The caret package ](http://topepo.github.io/caret/index.html)

* Copy/paste from cran R [ranger package](https://cran.r-project.org/web/packages/ranger/ranger.pdf)


### Models

* Random Forest (ranger)

### Non-standard R packages

* ggplot2
* VIM
* fasttime
* lubridate
* PCAmixdata
* lattice
* corrplot
* ROSE
* caret
* elasticnet
* pROC
* ranger
* ggRandomForest
* randomForestSCR


```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, comment=NA, cache=F, R.options=list(width=220))
```


### R code to reproduce the last submission:

```{r}
library(ggplot2)
library(VIM)
library(PCAmixdata)
library(fasttime)
library(lubridate)
library(car)
library(MASS)
library(lattice)
library(corrplot)
library(ROSE)
library(ranger)
library(elasticnet)
library(pROC)
library(randomForestSRC)
library(ggRandomForests)
library(caret)

train_okc <- read.csv("http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/101.csv",
                      stringsAsFactors = T)

n <- nrow(train_okc)

test_okc <- read.csv("http://bee-fore.s3-eu-west-1.amazonaws.com/datasets/102.csv", 
                     stringsAsFactors = T)
m <- nrow(test_okc)

train_okc <- train_okc[, -c(21:28)] #variabili con 0 varianza

miss <- sum(which(is.na(train_okc))) #0 missing

#round(table(train_okc$Class)/n * 100) #dataset sbilanciato 82% NO STEM, 18% STEM

train_okc$Class <- as.factor(train_okc$Class)
train_okc$education <- as.factor(train_okc$education)
train_okc$male <- as.factor(train_okc$male)
train_okc$essay_link <- as.factor(train_okc$essay_link)
#summary(glm(Class~technology+math+science+electronic, family = binomial, train_okc))

test_okc <- test_okc[, -c(21:28)]
test_okc$Class <- NA

for( i in 24:83){
  train_okc[,i] <- as.factor(train_okc[,i])
}

for( i in 24:83){
  test_okc[,i] <- as.factor(test_okc[,i])
}
test_okc$tech <- as.factor(test_okc$tech)
test_okc$education <- as.factor(test_okc$education)
test_okc$male <- as.factor(test_okc$male)
test_okc$essay_link <- as.factor(test_okc$essay_link)

combi <- rbind(train_okc, test_okc)

train_okc <- combi[1:4000,]

# Bilanciamento dataset ----------------------------------------------------------------------------

#divisione training test e test set
set.seed(123)
n<-round(nrow(train_okc)*0.8)
sel<-sample(1:nrow(train_okc),n,rep=F)
train<-train_okc[sel,]
rownames(train)<-1:n
test<-train_okc[-sel,]

# mod <- glm(Class~ .,
#            train_okc, family=binomial)
# 
# outlierTest(mod)
# influencePlot(mod)

train_okc <- train_okc[-1677,] #rimozione di outlier e influenti

### Modelli ------------------------------------------------------------------------------------

# RANDOM FOREST RANGER -------------------------------------------------------------------------

ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = "down"
)
tgrid <- expand.grid(
  .mtry = 1:10,
  .splitrule = c("gini","extratrees"),
  .min.node.size = c(60,70,80,90)
)

set.seed(123)

fit_di <- caret::train(Class ~.,
                data = train, 
                method = "ranger",
               tuneGrid = tgrid,
                metric = "ROC",
                trControl = ctrl)

ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = "up"
)
tgrid <- expand.grid(
  .mtry = 1:10,
  .splitrule = c("gini","extratrees"),
  .min.node.size = c(60,70,80,90)
)

phat_di = predict(fit_di, test,  type = "prob")[,"stem"]

# obj_roc1<-roc.curve(test$Class, phat_di, plotit = T, col=2, lwd=2)
# roc(response = test$Class,predictor = phat_di,levels = c("stem","other"))$auc 
# legend("bottomright", legend=round(obj_roc1$auc,3), col=2, lwd=2)

#Previsione:
ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = "down"
)
tgrid <- expand.grid(
  .mtry = 1:10,
  .splitrule = c("gini","extratrees"),
  .min.node.size = c(60,70,80,90)
)

set.seed(123)

fit_di <- caret::train(Class ~.,
                data = train_okc, 
                method = "ranger",
                tuneGrid = tgrid,
                metric = "ROC",
                trControl = ctrl)

phat <- predict(fit_di,test_okc,type="prob")[,"stem",drop=F]

# show first 6 predicted values
head(phat)
```


